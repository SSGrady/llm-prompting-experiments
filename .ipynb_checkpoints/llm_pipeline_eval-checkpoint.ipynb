{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentication\n",
    "Loads variables from `.env`:\n",
    "- `LLM_API_KEY`: **REQUIRED** Open Router API Key (other LLM providers are valid if the BASE_URL matches...)\n",
    "- `LLM_BASE_URL`: Defaults to `https://openrouter.ai/api/v1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from textwrap import shorten\n",
    "from dotenv import load_dotenv\n",
    "from master_agent.llm import LlmClient, SubtasksGenerator\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "llm_api_key = os.getenv('LLM_API_KEY')\n",
    "assert llm_api_key != None\n",
    "llm_base_url = os.getenv('LLM_BASE_URL', 'https://openrouter.ai/api/v1')\n",
    "\n",
    "print(f\"LLM API Key: {shorten(llm_api_key, width=25, placeholder='...')}\")\n",
    "print(f\"LLM Base URL: {llm_base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Testing Dataset\n",
    "## Parameters:\n",
    "- **COLOR_NAMES**: These are the Object Colors provided from the Minigrid constants\n",
    "- **NUM_TRAILS**: The number of trials (example envs) to be tested\n",
    "- **MAX_OBJECTS**: The maximum number of object pairs generated from the environment\n",
    "- **MIN_OBJECTS**: The miniumum number of object pairs generated from the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_NAMES = sorted([\"red\", \"green\", \"blue\", \"purple\", \"yellow\"])\n",
    "NUM_TRIALS = 10\n",
    "MAX_OBJECTS = 3\n",
    "MIN_OBJECTS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def generate_random_objects(num_objects: int) -> list[str]:\n",
    "    objects = []\n",
    "    for _ in range(num_objects):\n",
    "        color = random.choice(COLOR_NAMES).capitalize()\n",
    "        objects.append(f\"Key{color}\")\n",
    "        objects.append(f\"Door{color}\")\n",
    "    return objects\n",
    "\n",
    "data_set = [generate_random_objects(random.randint(MIN_OBJECTS, MAX_OBJECTS)) for _ in range(NUM_TRIALS)]\n",
    "df = pd.DataFrame(data_set)\n",
    "print(\"Generated Testing Dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Test\n",
    "## Parameters\n",
    "- **RUN_PARALLEL**: Runs the model tests for each model in **AVAILABLE_MODELS** in parallel\n",
    "- **AVAILABLE_MODELS**: The models to be tested\n",
    "## Outputs\n",
    "- Graph #1: Compares model's success rates\n",
    "- Graph #2: Compares model's response times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_PARALLEL = True\n",
    "AVAILABLE_MODELS = [\n",
    "    \"openai/gpt-4o\",\n",
    "    \"anthropic/claude-3.5-sonnet\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import concurrent.futures\n",
    "import time\n",
    "import datetime\n",
    "from colorama import Fore, Style, init as colorama_init\n",
    "import hashlib\n",
    "colorama_init(autoreset=True)\n",
    "\n",
    "objects_list = df.apply(lambda row: [obj for obj in row if obj is not None], axis=1).tolist()\n",
    "\n",
    "def get_color_from_string(s: str) -> str:\n",
    "    \"\"\"Generate a consistent color from a string using hashing\"\"\"\n",
    "    hash_object = hashlib.md5(s.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    # Use first 6 characters of hash for RGB color\n",
    "    return f\"#{hash_hex[:6]}\"\n",
    "\n",
    "def test_model(model: str, objects_list: list) -> tuple:\n",
    "    \"\"\"\n",
    "    For the given model, run tests for each set of objects, printing logs with:\n",
    "       - Timestamp\n",
    "       - Colored status (green for Passed, red for Failed)\n",
    "       - Current success rate (so far)\n",
    "       - Current average test duration (so far)\n",
    "    \n",
    "    Returns a tuple: (model, successes, failures, avg_duration)\n",
    "    \"\"\"\n",
    "    successes = 0\n",
    "    failures = 0\n",
    "    durations = []  # list to record the duration of each test iteration\n",
    "\n",
    "    # Instantiate client and subtasks generator for the model\n",
    "    llm_client = LlmClient(llm_api_key, model, llm_base_url)\n",
    "    subtasks_gen = SubtasksGenerator(llm_client)\n",
    "\n",
    "    for i, objects in enumerate(objects_list):\n",
    "        start_time = time.time()\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        try:\n",
    "            subtasks_gen.gen_subtask_paths(objects)\n",
    "            successes += 1\n",
    "            result_text = \"Passed\"\n",
    "            result_color = Fore.GREEN\n",
    "        except Exception:\n",
    "            failures += 1\n",
    "            result_text = \"Failed\"\n",
    "            result_color = Fore.RED\n",
    "\n",
    "        iteration_duration = time.time() - start_time\n",
    "        durations.append(iteration_duration)\n",
    "        current_success_rate = (successes / (i + 1)) * 100\n",
    "        current_avg_duration = sum(durations) / len(durations)\n",
    "\n",
    "        print(f\"{timestamp} | [{model}] iteration #{i+1} \"\n",
    "              f\"{result_color}{result_text}{Style.RESET_ALL} | \"\n",
    "              f\"Current success rate: {current_success_rate:.2f}% | \"\n",
    "              f\"Current avg duration: {current_avg_duration:.2f}s\")\n",
    "\n",
    "    avg_duration = sum(durations) / len(durations) if durations else 0\n",
    "    return model, successes, failures, avg_duration\n",
    "\n",
    "results = []  # will store tuples: (model, successes, failures, avg_duration)\n",
    "\n",
    "if RUN_PARALLEL:\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(AVAILABLE_MODELS)) as executor:\n",
    "        future_to_model = {\n",
    "            executor.submit(test_model, model, objects_list): model\n",
    "            for model in AVAILABLE_MODELS\n",
    "        }\n",
    "        for future in concurrent.futures.as_completed(future_to_model):\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "else:\n",
    "    for model in AVAILABLE_MODELS:\n",
    "        result = test_model(model, objects_list)\n",
    "        results.append(result)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Print summary and collect data for graphs\n",
    "# -----------------------------------------------------------------------------\n",
    "print(\"\\nSummary:\")\n",
    "model_names = []\n",
    "success_rates = []\n",
    "avg_durations = []\n",
    "\n",
    "for model, successes, failures, avg_duration in results:\n",
    "    total = successes + failures\n",
    "    success_rate = (successes / total * 100) if total > 0 else 0\n",
    "    print(f\"{model}: {successes}/{total} passed ({success_rate:.2f}%), \"\n",
    "          f\"Avg test duration: {avg_duration:.2f}s\")\n",
    "    model_names.append(model)\n",
    "    success_rates.append(success_rate)\n",
    "    avg_durations.append(avg_duration)\n",
    "\n",
    "# Generate colors based on model names\n",
    "model_colors = [get_color_from_string(model) for model in model_names]\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Graph 1: Success Rates per Model\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars1 = plt.bar(model_names, success_rates, color=model_colors)\n",
    "plt.xlabel('LLM Models')\n",
    "plt.ylabel('Success Rate (%)')\n",
    "plt.title('Success Rates per LLM Model')\n",
    "plt.ylim(0, 100)\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, rate in zip(bars1, success_rates):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 2, f'{rate:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Graph 2: Average Test Duration per Model\n",
    "# -----------------------------------------------------------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "bars2 = plt.bar(model_names, avg_durations, color=model_colors)\n",
    "plt.xlabel('LLM Models')\n",
    "plt.ylabel('Average Test Duration (s)')\n",
    "plt.title('Average Test Duration per LLM Model')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "for bar, duration in zip(bars2, avg_durations):\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width() / 2, yval + 0.05, f'{duration:.2f}s', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gymnasium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
